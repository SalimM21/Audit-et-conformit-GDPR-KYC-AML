# üìã **GUIDE LOGGING CENTRALIS√â**

*MLOps Scoring Platform - Loki + Promtail + Grafana*
*Collecte, Stockage, Analyse et Visualisation des Logs*

---

## üìã **APER√áU**

Ce guide pr√©sente le syst√®me complet de logging centralis√© pour la plateforme MLOps Scoring, utilisant Loki pour le stockage, Promtail pour la collecte, et Grafana pour la visualisation et l'analyse avanc√©e des logs.

### **Capacit√©s du Syst√®me**
- ‚úÖ **Collecte centralis√©e** : Tous les composants (API, DB, ML, Infra)
- ‚úÖ **Stockage optimis√©** : Loki avec compression et indexation
- ‚úÖ **Recherche avanc√©e** : LogQL pour requ√™tes complexes
- ‚úÖ **Alertes bas√©es logs** : D√©tection patterns et anomalies
- ‚úÖ **Visualisation Grafana** : Dashboards interactifs et temps r√©el
- ‚úÖ **Analyse automatis√©e** : Scripts d'analyse et rapports
- ‚úÖ **Archivage intelligent** : R√©tention et compression automatique

---

## üèóÔ∏è **ARCHITECTURE DU LOGGING**

### **Composants de la Stack**

#### **1. Loki (Stockage)**
```yaml
# Stockage distribu√© et optimis√© pour logs
- Compression: gzip/zstd
- Indexation: par labels et timestamps
- R√©tention: configurable (7-90 jours)
- Haute disponibilit√©: r√©plication
- API: RESTful pour requ√™tes
```

#### **2. Promtail (Collecte)**
```yaml
# Agent de collecte l√©ger
- D√©couverte Kubernetes automatique
- Parsing intelligent par composant
- Labels enrichis automatiquement
- Filtrage et transformation
- Haute performance: faible overhead
```

#### **3. Grafana (Visualisation)**
```yaml
# Interface d'analyse avanc√©e
- Requ√™tes LogQL natives
- Dashboards interactifs
- Alertes int√©gr√©es
- Exports et partages
- Plugins et extensions
```

### **Flux de Donn√©es**
```
Applications ‚Üí Promtail ‚Üí Loki ‚Üí Grafana ‚Üí Utilisateurs
       ‚Üì           ‚Üì        ‚Üì        ‚Üì
   Logs bruts  Parsing  Indexation  Requ√™tes
   structur√©s  Enrichi   Compress√©  Visualis√©es
```

---

## üöÄ **D√âPLOIEMENT DU LOGGING CENTRALIS√â**

### **1. Cr√©er le Namespace**
```bash
kubectl create namespace logging
```

### **2. D√©ployer Loki**
```bash
kubectl apply -f logging/loki-config.yaml
kubectl apply -f logging/loki-pvc.yaml

# V√©rifier le d√©ploiement
kubectl get pods -n logging
kubectl logs -f deployment/loki -n logging
```

### **3. D√©ployer Promtail**
```bash
kubectl apply -f logging/promtail-config.yaml

# V√©rifier la collecte
kubectl get pods -n logging
kubectl logs -f ds/promtail -n logging
```

### **4. Configurer Grafana**
```bash
# Ajouter Loki comme source de donn√©es
curl -X POST http://grafana:3000/api/datasources \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Loki",
    "type": "loki",
    "url": "http://loki.logging.svc.cluster.local:3100",
    "access": "proxy"
  }'

# Importer le dashboard
curl -X POST http://grafana:3000/api/dashboards/import \
  -H "Content-Type: application/json" \
  -d @logging/logging-dashboards.json
```

### **5. Configurer les Alertes**
```bash
# Ajouter les r√®gles Loki au Ruler
kubectl apply -f logging/loki-alerts.yaml

# Red√©marrer Loki pour prendre en compte les r√®gles
kubectl rollout restart deployment/loki -n logging
```

---

## üìä **COLLECTE DES LOGS PAR COMPOSANT**

### **Logs Applicatifs**

#### **Scoring API**
```yaml
# Parsing structur√© JSON
pipeline_stages:
  - json:
      expressions:
        level: level
        timestamp: timestamp
        message: message
        request_id: request_id
        user_id: user_id
        model_version: model_version
  - labels:
      level:
      component: scoring-api
      request_id:
      user_id:
      model_version:
```

#### **API Gateway**
```yaml
# Parsing regex pour logs texte
pipeline_stages:
  - regex:
      expression: '^(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) \[(?P<level>\w+)\] (?P<message>.+)$'
  - labels:
      level:
      component: api-gateway
```

### **Logs Infrastructure**

#### **Kafka**
```yaml
# Parsing complexe avec threads
pipeline_stages:
  - regex:
      expression: '(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3}) (?P<level>\w+) \[(?P<thread>[^\]]+)\] (?P<logger>[^\s]+) - (?P<message>.+)'
  - labels:
      level:
      thread:
      logger:
      component: kafka
```

#### **PostgreSQL**
```yaml
# Parsing base de donn√©es
pipeline_stages:
  - regex:
      expression: '(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{3}) (?P<pid>\d+) (?P<level>\w+)  (?P<message>.+)'
  - labels:
      level:
      pid:
      component: postgresql
```

### **Logs S√©curit√©**

#### **Keycloak**
```yaml
# Parsing s√©curit√© avec m√©thodes
pipeline_stages:
  - regex:
      expression: '(?P<timestamp>\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d{3}Z) (?P<level>\w+) \[(?P<class>[^\]]+)\] \((?P<method>[^\)]+)\) (?P<message>.+)'
  - labels:
      level:
      class:
      method:
      component: keycloak
```

---

## üîç **REQU√äTES LOGQL AVANC√âES**

### **Requ√™tes de Base**
```logql
# Tous les logs d'un composant
{component="scoring-api"}

# Logs d'erreur uniquement
{level="ERROR"}

# Logs d'un pod sp√©cifique
{pod_name="scoring-api-12345-abcde"}
```

### **Filtrage et Recherche**
```logql
# Recherche de texte
{component="scoring-api"} |= "timeout"

# Recherche insensible √† la casse
{component="scoring-api"} |~ "Timeout|TIMEOUT"

# Exclusion de patterns
{component="scoring-api"} != "health check"
```

### **Parsing et Extraction**
```logql
# Extraction JSON
{component="scoring-api"} | json | user_id=`{{.user_id}}`

# Parsing regex
{component="api-gateway"} | regex "(?P<method>\\w+) (?P<path>\\S+) (?P<status>\\d+)"

# Ligne format√©e
{component="scoring-api"} | line_format "{{.timestamp}} [{{.level}}] {{.message}}"
```

### **Agr√©gation et Analyse**
```logql
# Comptage par niveau
sum(count_over_time({component="scoring-api"}[1h])) by (level)

# Taux d'erreur
sum(rate({level="ERROR"}[5m])) / sum(rate({component=~".*"}[5m])) * 100

# Top erreurs
topk(10, sum(rate({level="ERROR"}[1h])) by (message))
```

### **Analyse Temporelle**
```logql
# Logs des derni√®res 24h
{component="scoring-api"} [24h]

# Comparaison p√©riodes
sum(rate({level="ERROR"}[1h])) / sum(rate({level="ERROR"}[1h] offset 24h))

# Tendances
deriv(sum(rate({component="scoring-api"}[5m]))[1h])
```

### **Requ√™tes Complexes**
```logql
# Erreurs corr√©l√©es
{component="scoring-api", level="ERROR"} and {component="postgresql", level="ERROR"}

# Analyse de performance
{component="scoring-api"} | json | latency > 5000

# S√©curit√©
{component="keycloak"} |~ "authentication.*failed" | json | ip=`{{.ip_address}}`
```

---

## üö® **ALERTES BAS√âES LOGS**

### **Alertes Applications**
```yaml
- alert: ScoringAPIErrorLogs
  expr: sum(rate({component="scoring-api", level=~"ERROR|FATAL|CRITICAL"}[5m])) > 0
  for: 2m
  labels: {severity: critical, component: scoring-api}
  annotations:
    summary: "Erreurs d√©tect√©es dans les logs Scoring API"

- alert: APIHighErrorRate
  expr: rate({component="api-gateway", level="ERROR"}[5m]) / rate({component="api-gateway"}[5m]) > 0.05
  for: 5m
  labels: {severity: warning, component: api-gateway}
  annotations:
    summary: "Taux d'erreur √©lev√© dans API Gateway"
```

### **Alertes Infrastructure**
```yaml
- alert: KafkaBrokerErrors
  expr: sum(rate({component="kafka", level="ERROR"}[5m])) by (pod_name) > 5
  for: 3m
  labels: {severity: critical, component: kafka}
  annotations:
    summary: "Erreurs Kafka d√©tect√©es"

- alert: DatabaseConnectionErrors
  expr: sum(rate({component="postgresql", message=~".*connection.*failed.*"}[5m])) > 0
  for: 2m
  labels: {severity: critical, component: postgresql}
  annotations:
    summary: "Erreurs de connexion base de donn√©es"
```

### **Alertes S√©curit√©**
```yaml
- alert: AuthenticationFailures
  expr: sum(rate({component="keycloak", message=~".*authentication.*failed.*"}[5m])) > 10
  for: 5m
  labels: {severity: warning, component: keycloak}
  annotations:
    summary: "√âchecs d'authentification multiples"

- alert: SecurityViolations
  expr: sum(rate({message=~".*unauthorized.*|.*forbidden.*"}[5m])) > 0
  for: 1m
  labels: {severity: critical, component: security}
  annotations:
    summary: "Violation de s√©curit√© d√©tect√©e"
```

### **Alertes Performance**
```yaml
- alert: SlowQueriesDetected
  expr: sum(rate({component="postgresql", message=~".*slow.*query.*"}[5m])) > 0
  for: 3m
  labels: {severity: warning, component: postgresql}
  annotations:
    summary: "Requ√™tes lentes d√©tect√©es"

- alert: HighLatencyDetected
  expr: sum(rate({component="scoring-api", message=~".*latency.*>.*5000.*"}[5m])) > 0
  for: 3m
  labels: {severity: warning, component: scoring-api}
  annotations:
    summary: "Latence √©lev√©e d√©tect√©e"
```

---

## üìà **DASHBOARDS GRAFANA**

### **Panneaux Disponibles**

#### **1. Log Volume Overview**
- Volume de logs par job/composant
- Tendances temporelles
- Comparaisons p√©riodes

#### **2. Error Rate by Component**
- Taux d'erreur par composant
- √âvolution dans le temps
- Seuils et alertes visuelles

#### **3. Recent Error Logs**
- Logs d'erreur en temps r√©el
- Formatage intelligent
- Liens vers contexte complet

#### **4. Application Logs Explorer**
- Exploration interactive
- Filtres dynamiques
- Recherche en temps r√©el

#### **5. Log Patterns Analysis**
- Analyse des patterns
- Statistiques par niveau
- D√©tection anomalies

#### **6. Security Events**
- √âv√©nements s√©curit√©
- Tentatives d'intrusion
- Violations d√©tect√©es

#### **7. Performance Issues**
- Probl√®mes performance
- Timeouts et lenteurs
- Erreurs m√©moire

#### **8. Business Logic Errors**
- Erreurs m√©tier
- Validations √©chou√©es
- Contraintes viol√©es

#### **9. Infrastructure Issues**
- Probl√®mes infrastructure
- Erreurs base de donn√©es
- D√©faillances r√©seau

#### **10. LogQL Query Builder**
- Guide des requ√™tes
- Exemples pratiques
- Aide √† la construction

#### **11. Log Metrics**
- M√©triques calcul√©es
- KPIs temps r√©el
- Indicateurs de sant√©

---

## üîç **ANALYSE AVANC√âE DES LOGS**

### **Script d'Analyse Automatis√©e**
```bash
cd logging

# √âtat g√©n√©ral des logs
./log-analysis.sh status all 1h

# Analyse des erreurs
./log-analysis.sh errors scoring-api 24h

# Analyse de performance
./log-analysis.sh performance all 6h

# Analyse de s√©curit√©
./log-analysis.sh security all 24h

# Analyse des tendances
./log-analysis.sh trends all 7d

# Recherche avanc√©e
./log-analysis.sh search "timeout" 1h

# Rapport complet
./log-analysis.sh report
```

### **Rapports Automatis√©s**
```markdown
# Rapport d'Analyse des Logs

## M√©triques Globales
- **Total logs**: 1,247,839 entr√©es
- **Logs/seconde**: 34.7 logs/s
- **Taux d'erreur**: 0.12%

## Analyse des Erreurs
### Top 10 Erreurs
1. **scoring-api**: timeout on model prediction...
2. **postgresql**: connection pool exhausted...
3. **kafka**: broker not available...

## Analyse Performance
- **Temps r√©ponse moyen**: 245ms
- **Taux succ√®s**: 99.88%
- **Timeouts d√©tect√©s**: 23

## Recommandations
- URGENT: Augmenter pool de connexions DB
- HAUTE: Optimiser mod√®le de scoring lent
- MOYENNE: Ajouter cache Redis pour features
```

---

## ‚öôÔ∏è **CONFIGURATION AVANC√âE**

### **R√©tention et Archivage**
```yaml
# Configuration Loki
table_manager:
  retention_deletes_enabled: true
  retention_period: 30d

# Archivage automatique
limits_config:
  max_query_length: 721h  # 30 jours
  reject_old_samples: true
  reject_old_samples_max_age: 168h  # 7 jours
```

### **Optimisations Performance**
```yaml
# Chunk size optimis√©
chunk_store_config:
  max_look_back_period: 0s

# Cache des r√©sultats
query_range:
  results_cache:
    cache:
      embedded_cache:
        enabled: true
        max_size_mb: 100

# Parall√©lisation
query_scheduler:
  max_outstanding_requests_per_tenant: 100
```

### **S√©curit√© et Conformit√©**
```yaml
# Chiffrement des logs sensibles
pipeline_stages:
  - replace:
      expression: "(password|token|key)\\s*=\\s*\\S+"
      replace: "$1=***"

# Audit logging
- labeldrop:
    - sensitive_data
- labels:
    audit_trail: enabled
    compliance: gdpr
```

### **Int√©grations Externes**
```yaml
# Export vers Elasticsearch
- job_name: elasticsearch-export
  loki_push_api:
    endpoint: http://elasticsearch:9200/_bulk

# Int√©gration SIEM
- job_name: siem-forward
  syslog:
    endpoint: splunk.company.com:514
    format: rfc3164
```

---

## üéØ **BONNES PRATIQUES**

### **Structuration des Logs**
```json
{
  "timestamp": "2025-11-13T13:50:00.000Z",
  "level": "INFO",
  "component": "scoring-api",
  "request_id": "req-12345",
  "user_id": "user-67890",
  "operation": "predict",
  "model_version": "v2.1.0",
  "latency_ms": 245,
  "status": "success",
  "message": "Prediction completed successfully"
}
```

### **Niveaux de Log Appropri√©s**
- **ERROR**: Erreurs impactant le service
- **WARN**: Probl√®mes potentiels, d√©gradations
- **INFO**: √âv√©nements business importants
- **DEBUG**: D√©tails techniques pour debugging

### **Labels Coh√©rents**
```yaml
# Labels standardis√©s
labels:
  component: scoring-api
  version: v2.1.0
  environment: production
  region: eu-west-1
  cluster: mlops-prod
```

### **Monitoring des Logs**
- **Volume**: D√©tecter anomalies de volume
- **Latence**: Monitorer ingestion Loki
- **Erreurs**: Alertes sur parsing failures
- **Stockage**: Gestion espace disque

---

## üìà **IMPACT BUSINESS**

### **Avantages Op√©rationnels**
- **D√©bogage acc√©l√©r√©** : Recherche instantan√©e dans tous les logs
- **D√©tection pr√©coce** : Patterns d'erreur identifi√©s automatiquement
- **R√©solution rapide** : Contexte complet pour troubleshooting
- **Observabilit√© compl√®te** : Vue 360¬∞ de la plateforme
- **Conformit√©** : Audit trails complets et s√©curis√©s

### **M√©triques de Succ√®s**
- **Temps MTTR** : -60% gr√¢ce √† recherche rapide
- **Couverture monitoring** : 100% composants track√©s
- **Alertes pertinentes** : < 5% faux positifs
- **Satisfaction √©quipes** : +80% (debugging facilit√©)
- **Conformit√©** : 100% logs audit√©s disponibles

### **ROI du Logging Centralis√©**
- **Productivit√© dev** : +40% (debugging acc√©l√©r√©)
- **Stabilit√© syst√®me** : +95% (d√©tection pr√©coce)
- **R√©solution incidents** : -70% temps MTTR
- **Conformit√©** : √âvitement p√©nalit√©s r√©glementaires
- **Innovation** : Focus sur features vs debugging

---

**üìã Logging centralis√© op√©rationnel !**

*Loki + Promtail + Grafana pour collecte, stockage et analyse*
*Logs structur√©s, alertes intelligentes, rapports automatis√©s* üéØ